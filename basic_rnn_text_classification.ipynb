{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import jieba \n",
    "import langid \n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data):\n",
    "\t''' This function is reading and preprocess the data '''\n",
    "\t\n",
    "\tword_insts = []\n",
    "\tlabels = []\n",
    "\twith open(data,encoding='utf-8') as f :\n",
    "\t\tfor sent in tqdm(f):\n",
    "\t\t\tfull_line = sent.split('\t')\n",
    "\t\t\tlabel = int(full_line[0])\n",
    "\t\t\tsent = full_line[1].replace(' ','')\n",
    "\t\t\tsent = sent.strip()\n",
    "\t\t\tsent_tuple = langid.classify(sent)\n",
    "\t\t\tif sent_tuple[0] == 'zh':\n",
    "\t\t\t\twords = list(jieba.cut(sent))\n",
    "\t\t\t\twords.insert(0,'<zh>')\n",
    "\t\t\t#elif sent_tuple[0] == 'ja':\n",
    "\t\t\t#\twords = (mecab.parse(sent)).split()\n",
    "\t\t\t\t#words.insert(0,'<ja>')\n",
    "\t\t\n",
    "\t\t\tif words and label:\n",
    "\t\t\t\tword_insts += [words]\n",
    "\t\t\t\tlabels.append(label)\n",
    "\t\n",
    "\treturn word_insts,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_idx(word_insts):\n",
    "\t''' Trim vocab by number of occurence '''\n",
    "\t\n",
    "\tfull_vocab = set(w for sent in word_insts for w in sent)\n",
    "\tprint('[Info] Original Vocabulary size =', len(full_vocab))\n",
    "\t\n",
    "\tword2idx = {\n",
    "\t\t'<pad>': 0,\n",
    "\t\t'<unk>': 1,\n",
    "\t\t'<zh>' : 2,\n",
    "\t\t'<ja>' : 3}\n",
    "\tword_count = {w: 0 for w in full_vocab}\n",
    "\t\n",
    "\tfor sent in word_insts:\n",
    "\t\tfor word in sent:\n",
    "\t\t\tword_count[word] += 1\n",
    "\t\t\t\n",
    "\tfor word,count in word_count.items():\n",
    "\t\tif word not in word2idx:\n",
    "\t\t\tword2idx[word] = len(word2idx)\n",
    "\treturn word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_instance_to_idx_seq(word_insts,word2idx):\n",
    "\t''' Mapping  words to idx sequence.'''\n",
    "\treturn [[word2idx.get(w,'<unk>') for w in s] for s in word_insts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(train_sec_insts, seq_length):\n",
    "\t''' Padding the insts'''\n",
    "\t\n",
    "\tfeatures = np.zeros((len(train_sec_insts), seq_length), dtype=int)\n",
    "\tfor i, row in enumerate(train_sec_insts):\n",
    "\t\tfeatures[i,:len(row)] = np.array(row)[:seq_length]\n",
    "\treturn features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features,labels):\n",
    "\t'''Split the dataset into train_data and val_data'''\n",
    "\t\n",
    "\tsplit_frac = 0.9\n",
    "\tsplit_idx = int(len(features) * split_frac)\n",
    "\ttrain_x, val_x = features[:split_idx], features[split_idx:]\n",
    "\ttrain_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\tprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "\t\t\"\\nValidation set: \\t{}\".format(val_x.shape))\n",
    "\treturn (train_x, train_y, val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(splited_data,batch_size):\n",
    "\t'''Turn the data into dataloader '''\n",
    "\t\n",
    "\ttrain_x, train_y, val_x, val_y = splited_data\n",
    "\ttrain_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
    "\tval_data = TensorDataset(torch.from_numpy(val_x),torch.from_numpy(val_y))\n",
    "\ttrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\tval_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "\treturn train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_label(labels):\n",
    "    label_dict = {}\n",
    "    label_dict[4] = 0\n",
    "    for i in range(1,len(labels)):\n",
    "        if labels[i] != labels[i-1] and labels[i] not in label_dict.keys():\n",
    "            label_dict[labels[i]] = len(label_dict)\n",
    "    new_label = []\n",
    "    for j in labels:\n",
    "        new_label.append(label_dict[j])\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(data,labels,batch_size):\n",
    "    ''' Turn the data into dataloader'''\n",
    "    \n",
    "    tensor_data = TensorDataset(torch.from_numpy(data),torch.from_numpy(labels))\n",
    "    data_loader = DataLoader(tensor_data,shuffle=True,batch_size=batch_size)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.950 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "57849it [01:34, 612.28it/s]\n",
      "1470it [00:02, 626.26it/s]\n"
     ]
    }
   ],
   "source": [
    "data = 'midea.train'\n",
    "train_insts,labels = read_file(data)\n",
    "#labels = np.array(labels)\n",
    "test_data = 'midea.test'\n",
    "test_insts,test_labels = read_file(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1470it [00:02, 649.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<zh>', '启动', '微波炉'], ['<zh>', '开', '一下', '微波炉'], ['<zh>', '开启', '微波炉'], ['<zh>', '开始', '使用', '微波炉'], ['<zh>', '微波炉', '启动'], ['<zh>', '微波炉', '开机'], ['<zh>', '我要', '用', '微波炉'], ['<zh>', '打开', '微波炉'], ['<zh>', '打开', '微波炉', '电源'], ['<zh>', '把', '微波炉', '开开']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = 'midea.test'\n",
    "test_insts,test_labels = read_file(test_data)\n",
    "print(test_insts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the train_insts is 57849\n",
      "The length of the test insts is 1470 \n",
      "The length of the train label is 57849\n",
      "The length of the test label is 1470\n"
     ]
    }
   ],
   "source": [
    "all_insts = train_insts + test_insts\n",
    "all_labels = labels + test_labels\n",
    "\n",
    "train_insts = all_insts[:len(train_insts)]\n",
    "train_labels = all_insts[:len(train_insts)]\n",
    "test_insts = all_insts[len(train_insts):]\n",
    "test_labels = all_labels[len(train_insts):]\n",
    "\n",
    "print('The length of the train_insts is {}'.format(len(train_insts)))\n",
    "print('The length of the test insts is {} '.format(len(test_insts)))\n",
    "print('The length of the train label is {}'.format(len(train_labels)))\n",
    "print('The length of the test label is {}'.format(len(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<zh>', '一定'], ['<zh>', '定个'], ['<zh>', '好', '的', '哎'], ['<zh>', '可以', '哟'], ['<zh>', '可以'], ['<zh>', '可以'], ['<zh>', '可以'], ['<zh>', '行', '吧'], ['<zh>', '行', '吧'], ['<zh>', '要', '确定']]\n",
      "[['<zh>', '启动', '微波炉'], ['<zh>', '开', '一下', '微波炉'], ['<zh>', '开启', '微波炉'], ['<zh>', '开始', '使用', '微波炉'], ['<zh>', '微波炉', '启动'], ['<zh>', '微波炉', '开机'], ['<zh>', '我要', '用', '微波炉'], ['<zh>', '打开', '微波炉'], ['<zh>', '打开', '微波炉', '电源'], ['<zh>', '把', '微波炉', '开开']]\n"
     ]
    }
   ],
   "source": [
    "print(all_insts[:10])\n",
    "print(test_insts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Original Vocabulary size = 7269\n",
      "Train set: \t(57849, 30) \n",
      "test set: \t(1470, 30)\n"
     ]
    }
   ],
   "source": [
    "word2idx = build_vocab_idx(all_insts)\n",
    "# train_sec_insts = convert_instance_to_idx_seq(train_insts,word2idx)\n",
    "# test_sec_insts = convert_instance_to_idx_seq(test_insts,word2idx)\n",
    "all_insts = convert_instance_to_idx_seq(all_insts,word2idx)\n",
    "np_all_label = turn_label(all_labels)\n",
    "# np_train_label = turn_label(train_labels)\n",
    "# np_test_label = turn_label(test_labels)\n",
    "train_sec_insts = all_insts[:len(train_insts)]\n",
    "test_sec_insts = all_insts[len(train_insts):]\n",
    "\n",
    "np_train_label = np_all_label[:len(train_insts)]\n",
    "np_test_label = np_all_label[len(train_insts):]\n",
    "\n",
    "np_train_label = np.array(np_train_label)\n",
    "np_test_label = np.array(np_test_label)\n",
    "\n",
    "\n",
    "train_feature = pad_features(train_sec_insts,30)\n",
    "test_feature = pad_features(test_sec_insts ,30)\n",
    "\n",
    "print(\"Train set: \\t{}\".format(train_feature.shape), \n",
    "\t\t\"\\ntest set: \\t{}\".format(test_feature.shape))\n",
    "\n",
    "train_loader = prepare_dataloader(train_feature, np_train_label, batch_size=50)\n",
    "test_loader = prepare_dataloader(test_feature, np_test_label, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.array(new_labels)\n",
    "# word2idx = build_vocab_idx(words_ints)\n",
    "# train_sec_insts = convert_instance_to_idx_seq(words_ints,word2idx)\n",
    "# features = pad_features(train_sec_insts,30)\n",
    "# splited_data = split_data(features,labels)\n",
    "# train_loader,val_loader = prepare_dataloader(splited_data,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 30])\n",
      "Sample input: \n",
      " tensor([[   2, 6299, 4915,  ...,    0,    0,    0],\n",
      "        [   2, 4746, 6566,  ...,    0,    0,    0],\n",
      "        [   2, 2754, 5417,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2, 2771,  368,  ...,    0,    0,    0],\n",
      "        [   2, 6299, 1215,  ...,    0,    0,    0],\n",
      "        [   2, 6299,  132,  ...,    0,    0,    0]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([64, 23, 85, 80, 27, 10, 47, 45, 13, 40, 73, 26, 84, 92, 72, 11, 47, 65,\n",
      "        20, 61, 95, 95, 25, 82, 41, 99, 25, 89, 43, 35, 10, 88, 11, 81, 34, 37,\n",
      "        31, 54, 17, 28, 97, 53, 78, 11, 17, 12, 71, 14, 90, 23])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "sample_x  , sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 30])\n",
      "Sample input: \n",
      " tensor([[   2,  149,  741,  ...,    0,    0,    0],\n",
      "        [   2, 1273, 6614,  ...,    0,    0,    0],\n",
      "        [   2, 7229, 6678,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2, 1961, 2430,  ...,    0,    0,    0],\n",
      "        [   2, 6516, 1784,  ...,    0,    0,    0],\n",
      "        [   2, 5943,  102,  ...,    0,    0,    0]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([ 59,   4,  59,  59,  83,   3,  65,  57,  47,  21,  11,  25,  57, 101,\n",
      "         90,  21,  27,  54,  84,   4,  73,  59,  16,  11,  59,  59,   3,  27,\n",
      "         60,  47,   3,  17,  17,  82,  12,  94,  94,  72,  41,   1,  57,  57,\n",
      "         94,  32,  17,  25,  28,  24,  11,  57])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x  , sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size, batch_size=50,num_layer=2,batch_first= True,\n",
    "                dropout=0.5, output_size=105, embedding_dim=300,train_on_gpu=True):\n",
    "        \n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.num_layer = num_layer \n",
    "        self.train_on_gpu = train_on_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim, padding_idx=0)\n",
    "        self.LSTM = nn.LSTM(input_size=embedding_dim,hidden_size=hidden_size,\n",
    "                           num_layers=num_layer,#,batch_first=batch_first)\n",
    "                           bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2,output_size)\n",
    "    \n",
    "    def init_hidden(self,batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        if self.train_on_gpu:\n",
    "            h0 = Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size).cuda())\n",
    "            c0 = Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size))\n",
    "            c0 = Variable(torch.zeros(self.num_layer*2, batch_size, self.hidden_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "#         batch_size = x.size(0)\n",
    "        \n",
    "#         hidden_state = Variable(torch.zeros(1, batch_size, self.hidden_size))   # [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "#         cell_state = Variable(torch.zeros(1,batch_size , self.hidden_size))  \n",
    "        \n",
    "#         hidden_state,cell_state = hidden_state.cuda(),cell_state.cuda()\n",
    "\n",
    "        \n",
    "        \n",
    "#         embed = self.embedding(x)\n",
    "        \n",
    "#         lstm_out ,(_,_) = self.LSTM(embed,(hidden_state,cell_state))\n",
    "#         lstm_out = lstm_out[:,-1,:]\n",
    "#         out = self.fc(lstm_out)\n",
    "#         #out = out[:,-1,:]\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        x = embeds.permute(1,0,2)\n",
    "        self.hidden = self.init_hidden(x.size()[0])\n",
    "        lstm_out,self.hidden = self.LSTM(x,self.hidden)\n",
    "        final = lstm_out[-1]\n",
    "        out = self.fc(final)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (embedding): Embedding(7272, 300, padding_idx=0)\n",
      "  (LSTM): LSTM(300, 256, num_layers=2, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=105, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx)\n",
    "net = LSTM(input_size=vocab_size,hidden_size=256)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 100... Loss: 3.581267...\n",
      "Epoch: 1/5... Step: 200... Loss: 3.765254...\n",
      "Epoch: 1/5... Step: 300... Loss: 2.970754...\n",
      "Epoch: 1/5... Step: 400... Loss: 3.035429...\n",
      "Epoch: 1/5... Step: 500... Loss: 2.724170...\n",
      "Epoch: 1/5... Step: 600... Loss: 2.683354...\n",
      "Epoch: 1/5... Step: 700... Loss: 1.886301...\n",
      "Epoch: 1/5... Step: 800... Loss: 1.888956...\n",
      "Epoch: 1/5... Step: 900... Loss: 1.511148...\n",
      "Epoch: 1/5... Step: 1000... Loss: 1.245008...\n",
      "Epoch: 1/5... Step: 1100... Loss: 0.708896...\n",
      "Epoch: 2/5... Step: 1200... Loss: 0.767166...\n",
      "Epoch: 2/5... Step: 1300... Loss: 0.417788...\n",
      "Epoch: 2/5... Step: 1400... Loss: 0.519082...\n",
      "Epoch: 2/5... Step: 1500... Loss: 0.521403...\n",
      "Epoch: 2/5... Step: 1600... Loss: 0.321520...\n",
      "Epoch: 2/5... Step: 1700... Loss: 0.382657...\n",
      "Epoch: 2/5... Step: 1800... Loss: 0.345871...\n",
      "Epoch: 2/5... Step: 1900... Loss: 0.124673...\n",
      "Epoch: 2/5... Step: 2000... Loss: 0.182428...\n",
      "Epoch: 2/5... Step: 2100... Loss: 0.308351...\n",
      "Epoch: 2/5... Step: 2200... Loss: 0.276433...\n",
      "Epoch: 2/5... Step: 2300... Loss: 0.211595...\n",
      "Epoch: 3/5... Step: 2400... Loss: 0.189184...\n",
      "Epoch: 3/5... Step: 2500... Loss: 0.113296...\n",
      "Epoch: 3/5... Step: 2600... Loss: 0.212519...\n",
      "Epoch: 3/5... Step: 2700... Loss: 0.175681...\n",
      "Epoch: 3/5... Step: 2800... Loss: 0.431349...\n",
      "Epoch: 3/5... Step: 2900... Loss: 0.116040...\n",
      "Epoch: 3/5... Step: 3000... Loss: 0.198666...\n",
      "Epoch: 3/5... Step: 3100... Loss: 0.149155...\n",
      "Epoch: 3/5... Step: 3200... Loss: 0.098380...\n",
      "Epoch: 3/5... Step: 3300... Loss: 0.201124...\n",
      "Epoch: 3/5... Step: 3400... Loss: 0.106339...\n",
      "Epoch: 4/5... Step: 3500... Loss: 0.051975...\n",
      "Epoch: 4/5... Step: 3600... Loss: 0.116670...\n",
      "Epoch: 4/5... Step: 3700... Loss: 0.229230...\n",
      "Epoch: 4/5... Step: 3800... Loss: 0.115333...\n",
      "Epoch: 4/5... Step: 3900... Loss: 0.016705...\n",
      "Epoch: 4/5... Step: 4000... Loss: 0.009184...\n",
      "Epoch: 4/5... Step: 4100... Loss: 0.055238...\n",
      "Epoch: 4/5... Step: 4200... Loss: 0.044186...\n",
      "Epoch: 4/5... Step: 4300... Loss: 0.034175...\n",
      "Epoch: 4/5... Step: 4400... Loss: 0.095678...\n",
      "Epoch: 4/5... Step: 4500... Loss: 0.113674...\n",
      "Epoch: 4/5... Step: 4600... Loss: 0.005796...\n",
      "Epoch: 5/5... Step: 4700... Loss: 0.076085...\n",
      "Epoch: 5/5... Step: 4800... Loss: 0.101525...\n",
      "Epoch: 5/5... Step: 4900... Loss: 0.005164...\n",
      "Epoch: 5/5... Step: 5000... Loss: 0.036265...\n",
      "Epoch: 5/5... Step: 5100... Loss: 0.018346...\n",
      "Epoch: 5/5... Step: 5200... Loss: 0.049279...\n",
      "Epoch: 5/5... Step: 5300... Loss: 0.016032...\n",
      "Epoch: 5/5... Step: 5400... Loss: 0.036839...\n",
      "Epoch: 5/5... Step: 5500... Loss: 0.179006...\n",
      "Epoch: 5/5... Step: 5600... Loss: 0.059964...\n",
      "Epoch: 5/5... Step: 5700... Loss: 0.093834...\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "print_every = 100\n",
    "if train_on_gpu:\n",
    "    net.cuda()\n",
    "    \n",
    "counter = 0\n",
    "epochs =  5\n",
    "clip=5\n",
    "\n",
    "net.train()\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        counter += 1\n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(),labels.cuda()\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output = net(inputs)\n",
    "        loss = criterion(output.squeeze(),labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        if counter % print_every == 0:\n",
    "#             test_losses = []\n",
    "#             net.eval()\n",
    "#             for inputs,labels in test_loader:\n",
    "#                 if(train_on_gpu):\n",
    "#                     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                \n",
    "#                 outputs = net(inputs)\n",
    "#                 test_loss = criterion(outputs.squeeze(),labels)\n",
    "#                 test_losses.append(test_loss.item())\n",
    "# #                 print(val_losses)\n",
    "#             net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()))#,\n",
    "#                   \"Val Loss: {:.6f}\".format(np.mean(test_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.116\n",
      "Test accuracy: 0.786\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "net.eval() \n",
    "\n",
    "test_losses = []\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "for inputs ,labels in test_loader:\n",
    "    \n",
    "    if  train_on_gpu:\n",
    "        inputs , labels = inputs.cuda() , labels.cuda()\n",
    "        \n",
    "    output = net(inputs)\n",
    "    test_loss = criterion(output.squeeze(),labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())\n",
    "#     print(pred.size())\n",
    "#     print(pred.data.max(1)[1])\n",
    "    pred = pred.data.max(1)[1]\n",
    "    pred_labels += list(pred.cpu().numpy())\n",
    "    true_labels += list(labels.cpu().numpy())\n",
    "    correct_tensor = pred.eq(labels).view_as(pred)\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 76, 17, 50, 17, 9, 85, 104, 88, 65]\n",
      "[27, 76, 17, 57, 17, 9, 85, 104, 88, 65]\n",
      "This test's F1 score is 0.7866241638925211\n"
     ]
    }
   ],
   "source": [
    "# print(pred_labels)\n",
    "# real_label = list(test_labels)\n",
    "print(true_labels[:10])\n",
    "print(pred_labels[:10])\n",
    "f1_score = metrics.f1_score(true_labels,pred_labels,average='weighted')\n",
    "print(\"This test's F1 score is {}\".format(f1_score))\n",
    "# print(metrics.f1_score(true_labels,pred_labels,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net,test_review,word2idx,seqence_length = 30,train_on_put=True):\n",
    "    \n",
    "    net.eval()\n",
    "    test_review = test_review.replace(' ','')\n",
    "    test_review = list(jieba.cut(test_review))\n",
    "    test_review.insert(0,'<zh>')\n",
    "    test_ints = [[word2idx[word] for word in test_review]]\n",
    "    \n",
    "    features = pad_features(test_ints,30)\n",
    "    \n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    output = net(feature_tensor)\n",
    "    print(output.data.max(1))\n",
    "    output = output.data.max(1)[1]\n",
    "    print(output.size())\n",
    "    print(output)\n",
    "    #pred = torch.round(output.squeeze())\n",
    "    \n",
    "    #print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "#     print(len(pred))\n",
    "#     print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([13.0319], device='cuda:0'), tensor([104], device='cuda:0'))\n",
      "torch.Size([1])\n",
      "tensor([104], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_review = '恢复 运行 一下'\n",
    "predict(net,test_review,word2idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
